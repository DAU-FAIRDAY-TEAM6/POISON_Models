{"cells":[{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":6108,"status":"ok","timestamp":1721366630518,"user":{"displayName":"최범규","userId":"05104210716344243257"},"user_tz":-540},"id":"fNaE73RP23kP"},"outputs":[],"source":["import time\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","\n","import math\n","\n","import csv\n","import pandas as pd\n","from tqdm import tqdm  # tqdm 라이브러리 임포트\n","from tqdm.auto import trange\n","import random\n","from multiprocessing import Pool\n","import pickle\n","from scipy.spatial.distance import pdist, squareform\n","from torch_geometric.nn import LGConv"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1721366630518,"user":{"displayName":"최범규","userId":"05104210716344243257"},"user_tz":-540},"id":"5klGMkPE28sE"},"outputs":[],"source":["def get_data(path = \"/content/drive/MyDrive/학교/졸업작품/\"):\n","\n","    business_info_file = path + 'business_info.csv' # 필라델피아 가게 정보 business_id, latitude, longitude, city, idx\n","\n","    business_location = []\n","    with open(business_info_file, 'r', newline='') as business_file:\n","        csv_reader = csv.reader(business_file)\n","        next(csv_reader)  # 헤더 행 건너뛰기\n","        for row in csv_reader:\n","            _, latitude, longitude, _, _ = row[0], row[1], row[2], row[3].lower(), row[4] #city : 소문자로 받음\n","\n","            business_location.append([latitude, longitude])\n","    business_location = np.array(business_location, dtype=float)\n","\n","    input_file = path + \"reviews.txt\"\n","    data = []\n","    with open(input_file, 'r', newline='', encoding='utf-8') as csv_file:\n","        csv_reader = csv.reader(csv_file)\n","        for row in csv_reader:\n","            data.append(row)\n","\n","    if not data:  # 데이터가 비어있는 경우 처리\n","        return\n","\n","    user_history_list = []\n","    user_reviews_list = []\n","    user_ratings_list = []\n","    user_review_emb_list = []\n","\n","    tmp_reviews = []\n","    tmp_ratings = []\n","    tmp_business_id = []\n","    tmp_review_emb_list = []\n","\n","    before_user_id = data[0][0]  # 첫 번째 사용자 ID로 초기화\n","    for idx, i in enumerate(data):\n","        user_id, business_id, rating, review = i\n","        check_rating = float(rating) > 3.0\n","        if user_id == before_user_id:\n","            tmp_business_id.append(int(business_id))\n","            tmp_ratings.append(float(rating))\n","            tmp_reviews.append(review)\n","            tmp_review_emb_list.append((idx,check_rating))\n","        else:\n","            if len(tmp_business_id) >= 10:  # 방문 횟수가 10회가 넘는 유저만 append\n","                user_history_list.append(tmp_business_id)\n","                user_ratings_list.append(tmp_ratings)\n","                user_reviews_list.append(tmp_reviews)\n","                user_review_emb_list.append(tmp_review_emb_list)\n","            tmp_business_id = [int(business_id)]\n","            tmp_ratings = [float(rating)]\n","            tmp_reviews = [review]\n","            tmp_review_emb_list = [(idx, check_rating)]\n","\n","            before_user_id = user_id  # 현재 사용자 ID로 업데이트\n","\n","    # 마지막 사용자 처리\n","    if len(tmp_business_id) >= 10:\n","        user_history_list.append(tmp_business_id)\n","        user_ratings_list.append(tmp_ratings)\n","        user_reviews_list.append(tmp_reviews)\n","        user_review_emb_list.append(tmp_review_emb_list)\n","    print(len(user_history_list), len(user_ratings_list), len(user_reviews_list), len(user_review_emb_list))\n","\n","\n","    # POI가 가진 리뷰 임베딩을 획득하기 위해\n","    # history_list를 기준으로 POI에 방문한 사람들 list 생성\n","    poi_visited_list = []\n","    for user,history in enumerate(user_history_list):\n","        for idx, poi in enumerate(history):\n","            poi_visited_list.append([int(user), int(poi), float(user_ratings_list[user][idx]), user_reviews_list[user][idx], user_review_emb_list[user][idx]])\n","\n","    poi_visited_list.sort(key = lambda x:x[1]) # poi 번호 순으로 정렬\n","\n","    item_history_list = []\n","    item_reviews_list = []\n","    item_ratings_list = []\n","    item_review_emb_list = []\n","\n","\n","    tmp_reviews = []\n","    tmp_ratings = []\n","    tmp_user_id = []\n","    tmp_review_emb = []\n","    before_poi_id = poi_visited_list[0][1]  # 첫 번째 POI ID로 초기화\n","\n","    for idx, i in enumerate(poi_visited_list):\n","        user_id, business_id, rating, review, review_emb = i[0], i[1], i[2], i[3], i[4]\n","        if business_id == before_poi_id: # 이전 POI Id와 동일하다면\n","            tmp_user_id.append(user_id)\n","            tmp_ratings.append(rating)\n","            tmp_reviews.append(review)\n","            tmp_review_emb.append(review_emb)\n","        else: # 이전 POI ID와 다른 POI라면\n","            #print(business_id)\n","            # 이전 POI 정보 안에 있던거 다 추가하고\n","            item_history_list.append(tmp_user_id)\n","            item_ratings_list.append(tmp_ratings)\n","            item_reviews_list.append(tmp_reviews)\n","            item_review_emb_list.append(tmp_review_emb)\n","\n","            if int(business_id) - int(before_poi_id) > 1:\n","                for _ in range(int(business_id) - int(before_poi_id) - 1):\n","                    #print(f\"방문 기록이 없는 POI는 PASS\")\n","                    item_history_list.append([])\n","                    item_ratings_list.append([])\n","                    item_reviews_list.append([])\n","                    item_review_emb_list.append([])\n","\n","            tmp_user_id = [user_id]\n","            tmp_ratings = [rating]\n","            tmp_reviews = [review]\n","            tmp_review_emb = [review_emb]\n","\n","            before_poi_id = business_id  # 현재 사용자 ID로 업데이트\n","\n","    item_history_list.append(tmp_business_id)\n","    item_ratings_list.append(tmp_ratings)\n","    item_reviews_list.append(tmp_reviews)\n","    item_review_emb_list.append(tmp_review_emb)\n","\n","    print(len(item_history_list), len(item_ratings_list), len(item_reviews_list), len(item_review_emb_list))\n","\n","\n","    embedding_file = path + 'embeddings.npy'\n","    embeddings = np.load(embedding_file, mmap_mode='r')\n","\n","    user_review_embs = []\n","    for poi, embeds in enumerate(user_review_emb_list):\n","        if len(embeds)>0: # 비어있지 않으면\n","            # 기존\n","            new_array = np.array([embeddings[idx] for idx, check_rating in embeds])\n","            new_array = np.mean(new_array, axis = 0)\n","\n","            # # 변경\n","            # temp_list = []\n","            # for idx, check_rating in embeds:\n","            #     if check_rating:\n","            #         temp_list.append(embeddings[idx])\n","            # if len(temp_list):\n","            #     new_array = np.array(temp_list)\n","            #     new_array = np.mean(new_array, axis = 0)\n","            # else:\n","            #     new_array = np.zeros(768, dtype=np.float32)\n","        else:\n","            new_array = np.zeros(768, dtype=np.float32)\n","        user_review_embs.append(new_array)\n","\n","    item_review_embs = []\n","    for poi, embeds in enumerate(item_review_emb_list):\n","        if len(embeds)>0: # 비어있지 않으면\n","            new_array = np.array([embeddings[idx] for idx, check_rating in embeds])\n","            new_array = np.mean(new_array, axis = 0)\n","        else:\n","            new_array = np.zeros(768, dtype=np.float32)\n","\n","        item_review_embs.append(new_array.tolist())\n","\n","    return user_history_list, user_ratings_list, user_reviews_list, user_review_embs, item_history_list, item_ratings_list, item_reviews_list, item_review_embs, business_location"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1721366630519,"user":{"displayName":"최범규","userId":"05104210716344243257"},"user_tz":-540},"id":"m6qRtumj3FXu"},"outputs":[],"source":["class Yelp(Dataset):\n","    def __init__(self):\n","        \"\"\"\n","        Yelp 데이터셋을 로드하고 학습 데이터와 테스트 데이터를 생성합니다.\n","\n","        Args:\n","            dir (str): 데이터 파일이 있는 디렉토리 경로.\n","            splitter (str): 파일에서 열을 구분하는 구분자.\n","            K (int): K 값, 즉 각 사용자마다 테스트에 사용되는 상호작용의 수.\n","        \"\"\"\n","        self.user_history, _, _, user_review_embeds ,_,_,_,poi_review_embeds, business_location = get_data()\n","        self.norm_distances = self.normalize_distances(self.calculate_distances(business_location))\n","        print(f\"Number of users: {len(self.user_history)}\")\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.train = []\n","        self.test = []\n","        self.poi_review_embeds = torch.tensor(poi_review_embeds).to(self.device)\n","        self.user_review_embeds = torch.tensor(user_review_embeds).to(self.device)\n","        #self.num_user = len(user_history_list)\n","        self.num_item = len(poi_review_embeds) # 14585\n","\n","        self.num_users = len(self.user_history)\n","        self.num_items = max(max(hist) for hist in self.user_history) + 1\n","\n","        self.train_data, self.test_data = self.split_data()\n","        self.train_edge_index = self.get_edge_index(self.train_data)\n","        self.test_edge_index = self.get_edge_index(self.test_data)\n","        print(f\"Number of users in train_data: {len(self.train_data)}\")\n","        print(f\"Number of users: {self.num_users}\")\n","        print(f\"Number of items: {self.num_items}\")\n","        print(f\"Edge index shape: {self.train_edge_index.shape}\")\n","        items = [i for i in range(self.num_item)]\n","        self.neg = dict()\n","\n","        random.seed(30)\n","        for u, hist in enumerate(self.user_history):\n","            random.shuffle(hist)\n","            self.train.append(hist[:int(len(hist) * 0.7)])\n","            self.test.append(hist[int(len(hist) * 0.7) :])\n","\n","            u_negs = set(items) - set(hist)\n","            self.neg[u] = list(u_negs) # ng dataset 생성\n","\n","        self.index_map = []\n","        for u, user_items in enumerate(self.train):\n","            for i in user_items:\n","                self.index_map.append((u, i))\n","\n","    def split_data(self):\n","        train_data = []\n","        test_data = []\n","        for user_hist in self.user_history:\n","            np.random.shuffle(user_hist)\n","            split = int(len(user_hist) * 0.8)\n","            train_data.append(user_hist[:split])\n","            test_data.append(user_hist[split:])\n","        return train_data, test_data\n","\n","    def get_edge_index(self, data):\n","        user_ids = []\n","        item_ids = []\n","        for user, items in enumerate(data):\n","            user_ids.extend([user] * len(items))\n","            item_ids.extend(items)\n","        edge_index = torch.tensor([user_ids, item_ids], dtype=torch.long)\n","        return edge_index.to(self.device)  # device를 지정해주세요\n","    \n","    def __len__(self):\n","        \"\"\"\n","        데이터셋의 사용자 수를 반환합니다.\n","        \"\"\"\n","        #return self.num_user\n","        return len(self.train_data)\n","\n","    def __getitem__(self, idx):\n","        # if idx >= len(self.train_data):\n","        #     raise IndexError(f\"Index {idx} out of range for train_data with length {len(self.train_data)}\")\n","        # pos_items = self.train_data[idx]\n","        # if not pos_items:\n","        #     pos_items = [random.randint(0, self.num_items - 1)]  # 빈 리스트인 경우 랜덤 아이템 선택\n","        # neg_items = list(set(range(self.num_items)) - set(self.user_history[idx]))\n","        # neg_item = random.choice(neg_items)\n","        # return idx, random.choice(pos_items), neg_item\n","        u, i = self.index_map[idx]\n","        # 부정적인 아이템 무작위 선택\n","        j = random.choice(self.neg[u])\n","        return (u, i, j)\n","    \n","    def haversine(self, lat1, lon1, lat2, lon2):\n","        R = 6371\n","        dlat = np.radians(lat2 - lat1)\n","        dlon = np.radians(lon1 - lon2)  # Note the change here\n","        a = np.sin(dlat / 2) ** 2 + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dlon / 2) ** 2\n","        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n","        d = R * c\n","        return d\n","\n","    def calculate_distances(self, poi_data):\n","        t = time.time()\n","        distances = squareform(pdist(poi_data, lambda u, v: self.haversine(u[0], u[1], v[0], v[1])))\n","        print(f\"calculate_distance time : {int(time.time()-t)}\")\n","        return distances\n","\n","    def normalize_distances(self, distances):\n","        min_d = np.min(distances)\n","        max_d = np.max(distances)\n","        norm_distances = 0.5 * (distances - min_d) / (max_d - min_d) + 0.5\n","        return norm_distances"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1721366630519,"user":{"displayName":"최범규","userId":"05104210716344243257"},"user_tz":-540},"id":"CXwAv248lIPG"},"outputs":[],"source":["def bpr_loss(user_emb, pos_item_emb, neg_item_emb, distance_ij, lambda_reg=1e-6):\n","    pos_scores = (user_emb * pos_item_emb).sum(dim=1)\n","    neg_scores = (user_emb * neg_item_emb).sum(dim=1)\n","\n","    loss = -torch.log(torch.sigmoid(distance_ij*(pos_scores - neg_scores))).sum()\n","    reg_loss = lambda_reg * (user_emb.norm(2).pow(2) +\n","                             pos_item_emb.norm(2).pow(2) +\n","                             neg_item_emb.norm(2).pow(2))\n","\n","    return loss + reg_loss"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":763,"status":"ok","timestamp":1721366631276,"user":{"displayName":"최범규","userId":"05104210716344243257"},"user_tz":-540},"id":"6fJvTf55zo0K"},"outputs":[],"source":["def evaluate(model, dataset, k_values=[5, 10, 20], text=False):\n","    model.eval()\n","\n","    device = next(model.parameters()).device\n","    user_review_embeds = dataset.user_review_embeds.to(device)\n","    poi_review_embeds = dataset.poi_review_embeds.to(device)\n","\n","    with torch.no_grad():\n","        if text:\n","            user_emb, item_emb = model(dataset.test_edge_index, user_review_embeds, poi_review_embeds)\n","        else:\n","            user_emb, item_emb = model(dataset.test_edge_index)\n","\n","    scores = torch.matmul(user_emb, item_emb.t())\n","\n","    results = {k: {'hit_ratio': 0, 'recall': 0, 'precision': 0} for k in k_values}\n","    num_users = 0\n","\n","    for user in range(dataset.num_users):\n","        test_items = dataset.test_data[user]\n","        if not test_items:\n","            continue\n","\n","        num_users += 1\n","        user_scores = scores[user].cpu().detach()\n","\n","        # train_user_history = dataset.train_data[user]\n","        # user_scores[train_user_history] = -np.inf\n","\n","        _, top_k_items = torch.topk(user_scores, max(k_values))\n","        \n","        for k in k_values:\n","            hit = len(set(top_k_items[:k].numpy()) & set(test_items))\n","            results[k]['hit_ratio'] += (hit > 0)\n","            results[k]['recall'] += hit / len(test_items)\n","            results[k]['precision'] += hit / k\n","\n","    for k in k_values:\n","        results[k]['hit_ratio'] /= num_users\n","        results[k]['recall'] /= num_users\n","        results[k]['precision'] /= num_users\n","\n","    return results"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["def train(model, dataset, num_epochs=100, batch_size=1024, lr=0.001, text=False, patience=20):\n","    device = next(model.parameters()).device\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n","\n","    user_review_embeds = dataset.user_review_embeds.to(device)\n","    poi_review_embeds = dataset.poi_review_embeds.to(device)\n","\n","    norm_distances = dataset.norm_distances\n","    \n","    # Lists to store metrics\n","    losses = []\n","    hit_ratios = {k: [] for k in [5, 10, 20]}\n","    recalls = {k: [] for k in [5, 10, 20]}\n","    precisions = {k: [] for k in [5, 10, 20]}\n","\n","    # Dictionary to store best metrics\n","    best_metrics = {k: {'recall': 0, 'precision': 0, 'hit_ratio': 0, 'epoch': 0} for k in [5, 10, 20]}\n","\n","    # Early stopping variables\n","    best_recall_10 = 0\n","    epochs_no_improve = 0\n","    early_stop = False\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0\n","\n","        for users, pos_items, neg_items in tqdm(data_loader):\n","            optimizer.zero_grad()\n","\n","            users = users.to(device)\n","            pos_items = pos_items.to(device)\n","            neg_items = neg_items.to(device)\n","            if text:\n","                user_emb, item_emb = model(dataset.train_edge_index, user_review_embeds, poi_review_embeds)\n","            else:\n","                user_emb, item_emb = model(dataset.train_edge_index)\n","\n","            user_emb = user_emb[users]\n","            pos_item_emb = item_emb[pos_items]\n","            neg_item_emb = item_emb[neg_items]\n","\n","            # distance\n","            distance_ij = torch.tensor(norm_distances[pos_items, neg_items]).to(device)\n","            \n","            loss = bpr_loss(user_emb, pos_item_emb, neg_item_emb, distance_ij)\n","\n","            loss.backward()\n","            total_loss += loss.item()\n","            optimizer.step()\n","\n","        avg_loss = total_loss / len(data_loader)\n","        losses.append(avg_loss)\n","        \n","        # 검증\n","        results = evaluate(model, dataset, text=text)\n","\n","        # print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n","        for k in results.keys():\n","            hit_ratios[k].append(results[k]['hit_ratio'])\n","            recalls[k].append(results[k]['recall'])\n","            precisions[k].append(results[k]['precision'])\n","\n","            # Update best metrics if current recall is better\n","            if results[k]['recall'] > best_metrics[k]['recall']:\n","                best_metrics[k]['recall'] = results[k]['recall']\n","                best_metrics[k]['precision'] = results[k]['precision']\n","                best_metrics[k]['hit_ratio'] = results[k]['hit_ratio']\n","                best_metrics[k]['epoch'] = epoch + 1\n","\n","            # print(f\"HR@{k}: {results[k]['hit_ratio']:.6f}, Recall@{k}: {results[k]['recall']:.6f}, Precision@{k}: {results[k]['precision']:.6f}\")\n","\n","        # Early stopping check\n","        # racall@10기준 더이상 안오르면 중단\n","        if results[10]['recall'] > best_recall_10:\n","            best_recall_10 = results[10]['recall']\n","            epochs_no_improve = 0\n","        else:\n","            epochs_no_improve += 1\n","\n","        if epochs_no_improve == patience:\n","            # print(f\"Early stopping triggered. No improvement in Recall@10 for {patience} epochs.\")\n","            early_stop = True\n","            break\n","\n","    # Print best metrics\n","    print(\"\\nBest performance:\")\n","    for k, metrics in best_metrics.items():\n","        print(f\"k={k} (Epoch {metrics['epoch']}):\")\n","        print(f\"  Best Recall@{k}: {metrics['recall']:.6f}\")\n","        print(f\"  Corresponding HR@{k}: {metrics['hit_ratio']:.6f}\")\n","        print(f\"  Corresponding Precision@{k}: {metrics['precision']:.6f}\")\n","\n","    if early_stop:\n","        print(f\"Training stopped early at epoch {epoch+1}\")\n","    else:\n","        print(\"Training completed for all epochs\")\n","\n","    return best_metrics"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1721366631276,"user":{"displayName":"최범규","userId":"05104210716344243257"},"user_tz":-540},"id":"mG8Zl91IhcMX"},"outputs":[],"source":["class LightGCN(nn.Module):\n","    def __init__(self, num_users, num_items, num_layers=1, embedding_dim=128):\n","        super().__init__()\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.num_layers = num_layers\n","\n","        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n","        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n","\n","        self.convs = nn.ModuleList([LGConv() for _ in range(num_layers)])\n","\n","        self.init_parameters()\n","\n","    def init_parameters(self):\n","        nn.init.xavier_normal_(self.user_embedding.weight, gain=0.1)\n","        nn.init.xavier_normal_(self.item_embedding.weight, gain=0.1)\n","\n","    def forward(self, edge_index):\n","        user_emb = self.user_embedding.weight\n","        item_emb = self.item_embedding.weight\n","\n","        # 양방향 엣지 생성\n","        row, col = edge_index\n","        edge_index = torch.cat([edge_index, torch.stack([col, row])], dim=1)\n","\n","        x = torch.cat([user_emb, item_emb], dim=0)\n","        emb_list = [x]\n","        for conv in self.convs:\n","            x = conv(x, edge_index)\n","            emb_list.append(x)\n","\n","        emb = torch.stack(emb_list, dim=1).mean(dim=1)\n","        user_emb, item_emb = torch.split(emb, [self.num_users, self.num_items], dim=0)\n","        return user_emb, item_emb"]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1721366631276,"user":{"displayName":"최범규","userId":"05104210716344243257"},"user_tz":-540},"id":"Tkud9-t0sENf"},"outputs":[],"source":["class TextLightGCN(nn.Module):\n","    def __init__(self, num_users, num_items, num_layers=1, embedding_dim=128, text_embedding_dim=768, alpha=0.1):\n","        super().__init__()\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.num_layers = num_layers\n","        self.alpha = alpha\n","\n","        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n","        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n","\n","        self.latent_convs = nn.ModuleList([LGConv() for _ in range(num_layers)])\n","        # self.text_convs = nn.ModuleList([LGConv() for _ in range(num_layers)])\n","\n","        self.init_parameters()\n","\n","    def init_parameters(self):\n","        nn.init.xavier_normal_(self.user_embedding.weight, gain=0.1)\n","        nn.init.xavier_normal_(self.item_embedding.weight, gain=0.1)\n","\n","    def forward(self, edge_index, user_text_emb, item_text_emb):\n","        user_emb = self.user_embedding.weight\n","        item_emb = self.item_embedding.weight\n","\n","        user_text_emb = user_text_emb / math.sqrt(768)\n","        item_text_emb = item_text_emb / math.sqrt(768)\n","\n","        # Create bidirectional edges\n","        row, col = edge_index\n","        edge_index = torch.cat([edge_index, torch.stack([col, row])], dim=1)\n","\n","        # Process latent embeddings through their own LightGCN\n","        latent_x = torch.cat([user_emb, item_emb], dim=0)\n","        latent_emb_list = [latent_x]\n","        for conv in self.latent_convs:\n","            latent_x = conv(latent_x, edge_index)\n","            latent_emb_list.append(latent_x)\n","        latent_emb = torch.stack(latent_emb_list, dim=1).mean(dim=1)\n","        latent_user_emb, latent_item_emb = torch.split(latent_emb, [self.num_users, self.num_items], dim=0)\n","\n","        # # Process text projections through their own LightGCN\n","        # text_x = torch.cat([user_text_proj, item_text_proj], dim=0)\n","        # text_emb_list = [text_x]\n","        # for conv in self.text_convs:\n","        #     text_x = conv(text_x, edge_index)\n","        #     text_emb_list.append(text_x)\n","        # text_emb = torch.stack(text_emb_list, dim=1).mean(dim=1)\n","        # text_user_emb, text_item_emb = torch.split(text_emb, [self.num_users, self.num_items], dim=0)\n","\n","        #print(latent_user_emb.shape, user_text_emb.shape)\n","        #print(latent_item_emb.shape, item_text_emb.shape)\n","\n","\n","        # Concatenate final user and item embeddings\n","        final_user_emb = torch.cat([self.alpha * latent_user_emb , (1-self.alpha) * user_text_emb], dim=1)\n","        final_item_emb = torch.cat([latent_item_emb, item_text_emb], dim=1)\n","\n","        return final_user_emb, final_item_emb"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":192568,"status":"ok","timestamp":1721366823840,"user":{"displayName":"최범규","userId":"05104210716344243257"},"user_tz":-540},"id":"xF-neqBr-J_k","outputId":"ecb7c64c-fe38-480c-9623-b6d4c0b07603"},"outputs":[{"name":"stdout","output_type":"stream","text":["37685 37685 37685 37685\n","14585 14585 14585 14585\n","Number of users: 37685\n","Number of users in train_data: 37685\n","Number of users: 37685\n","Number of items: 14585\n","Edge index shape: torch.Size([2, 470029])\n"]}],"source":["np.random.seed(30)\n","yelp = Yelp()"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1721366823840,"user":{"displayName":"최범규","userId":"05104210716344243257"},"user_tz":-540},"id":"cij8g_bdd81d","outputId":"a7fc01df-9d31-4d76-81be-f871e3aa46ac"},"outputs":[{"data":{"text/plain":["37685"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["# 기존 한 유저당 1개의 긍정, 부정 데이터셋 #406548\n","len(yelp)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training with learning rate: 0.01\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1177/1177 [00:25<00:00, 46.03it/s]\n","100%|██████████| 1177/1177 [00:24<00:00, 48.28it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 49.53it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 50.46it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 50.73it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 50.64it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 50.75it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 49.10it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 49.10it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 49.35it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 49.20it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 49.26it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 49.76it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 49.52it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 49.27it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 49.31it/s]\n","100%|██████████| 1177/1177 [00:24<00:00, 48.89it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 49.15it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 49.24it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 49.29it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 49.37it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 49.46it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 49.26it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 50.44it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 50.56it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 50.87it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 50.28it/s]\n","100%|██████████| 1177/1177 [00:24<00:00, 48.29it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 50.50it/s]\n","100%|██████████| 1177/1177 [00:22<00:00, 51.28it/s]\n","100%|██████████| 1177/1177 [00:23<00:00, 50.64it/s]\n"," 45%|████▍     | 528/1177 [00:10<00:13, 49.80it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[51], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m lr_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.005\u001b[39m, \u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.0005\u001b[39m, \u001b[38;5;241m0.0001\u001b[39m]\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# 기본 LightGCN\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m best_lr_lightgcn \u001b[38;5;241m=\u001b[39m hyperparameter_tuning(LightGCN, yelp, lr_values, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# TextLightGCN\u001b[39;00m\n\u001b[0;32m     44\u001b[0m best_lr_textlightgcn \u001b[38;5;241m=\u001b[39m hyperparameter_tuning(TextLightGCN, yelp, lr_values, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","Cell \u001b[1;32mIn[51], line 9\u001b[0m, in \u001b[0;36mhyperparameter_tuning\u001b[1;34m(model_class, dataset, lr_values, num_epochs, batch_size, text)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining with learning rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(dataset\u001b[38;5;241m.\u001b[39mnum_users, dataset\u001b[38;5;241m.\u001b[39mnum_items)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m best_metrics \u001b[38;5;241m=\u001b[39m train(model, dataset, num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, lr\u001b[38;5;241m=\u001b[39mlr, text\u001b[38;5;241m=\u001b[39mtext)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, metrics \u001b[38;5;129;01min\u001b[39;00m best_metrics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     12\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: model_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: lr,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     20\u001b[0m     })\n","Cell \u001b[1;32mIn[46], line 45\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataset, num_epochs, batch_size, lr, text, patience)\u001b[0m\n\u001b[0;32m     42\u001b[0m     loss \u001b[38;5;241m=\u001b[39m bpr_loss(user_emb, pos_item_emb, neg_item_emb)\n\u001b[0;32m     44\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 45\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     46\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     48\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["def hyperparameter_tuning(model_class, dataset, lr_values, num_epochs, batch_size, text):\n","    results = []\n","    best_lr = None\n","    best_recall_10 = 0\n","    \n","    for lr in lr_values:\n","        print(f\"\\nTraining with learning rate: {lr}\")\n","        model = model_class(dataset.num_users, dataset.num_items).to('cuda' if torch.cuda.is_available() else 'cpu')\n","        best_metrics = train(model, dataset, num_epochs=num_epochs, batch_size=batch_size, lr=lr, text=text)\n","        \n","        for k, metrics in best_metrics.items():\n","            results.append({\n","                'model': model_class.__name__,\n","                'lr': lr,\n","                'k': k,\n","                'recall': metrics['recall'],\n","                'precision': metrics['precision'],\n","                'hit_ratio': metrics['hit_ratio'],\n","                'epoch': metrics['epoch']\n","            })\n","            \n","            # Recall@10을 기준으로 최고의 lr 저장\n","            if k == 10 and metrics['recall'] > best_recall_10:\n","                best_recall_10 = metrics['recall']\n","                best_lr = lr\n","    \n","    # Convert results to DataFrame\n","    df = pd.DataFrame(results)\n","    \n","    # Save results to CSV\n","    filename = f\"{model_class.__name__}_results.csv\"\n","    df.to_csv(filename, index=False)\n","    print(f\"Results saved to {filename}\")\n","    \n","    return best_lr\n","\n","# 학습률 값들 정의\n","lr_values = [0.01, 0.005, 0.001, 0.0005, 0.0001]\n","\n","# 기본 LightGCN\n","best_lr_lightgcn = hyperparameter_tuning(LightGCN, yelp, lr_values, num_epochs=100, batch_size=32, text=False)\n","\n","# TextLightGCN\n","best_lr_textlightgcn = hyperparameter_tuning(TextLightGCN, yelp, lr_values, num_epochs=300, batch_size=32, text=True)\n","\n","print(f\"Best learning rate for LightGCN: {best_lr_lightgcn}\")\n","print(f\"Best learning rate for TextLightGCN: {best_lr_textlightgcn}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def hyperparameter_tuning_alpha(dataset, alpha_values, num_epochs=300, batch_size=32, lr=0.01):\n","    results = []\n","    \n","    for alpha in alpha_values:\n","        print(f\"\\nTraining with alpha: {alpha}\")\n","        model = TextLightGCN(dataset.num_users, dataset.num_items, alpha=alpha).to('cuda' if torch.cuda.is_available() else 'cpu')\n","        \n","        # train 함수를 수정하여 alpha 값을 전달할 수 있도록 합니다.\n","        best_metrics = train(model, dataset, num_epochs=num_epochs, batch_size=batch_size, lr=lr, text=True)\n","        \n","        for k, metrics in best_metrics.items():\n","            results.append({\n","                'model': 'TextLightGCN',\n","                'alpha': alpha,\n","                'k': k,\n","                'recall': metrics['recall'],\n","                'precision': metrics['precision'],\n","                'hit_ratio': metrics['hit_ratio'],\n","                'epoch': metrics['epoch'],\n","                'lr': lr\n","            })\n","    \n","    # Convert results to DataFrame\n","    df = pd.DataFrame(results)\n","    \n","    # Save results to CSV\n","    filename = \"TextLightGCN_alpha_tuning_results.csv\"\n","    df.to_csv(filename, index=False)\n","    print(f\"Results saved to {filename}\")\n","\n","# alpha 값들 정의\n","alpha_values = [0.1, 0.5, 0.9]\n","\n","# TextLightGCN에 대한 alpha 튜닝 실행\n","hyperparameter_tuning_alpha(yelp, alpha_values, num_epochs=300, batch_size=32, lr=best_lr_textlightgcn)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def hyperparameter_tuning_embedding_dim(model_class, dataset, embedding_dims, num_epochs=300, batch_size=32, lr=0.01, text=True):\n","    results = []\n","    \n","    for embedding_dim in embedding_dims:\n","        print(f\"\\nTraining with embedding dimension: {embedding_dim}\")\n","        model = model_class(dataset.num_users, dataset.num_items, embedding_dim=embedding_dim).to('cuda' if torch.cuda.is_available() else 'cpu')\n","        \n","        # train 함수를 수정하여 embedding_dim 값을 전달할 수 있도록 합니다.\n","        best_metrics = train(model, dataset, num_epochs=num_epochs, batch_size=batch_size, lr=lr, text=text)\n","        \n","        for k, metrics in best_metrics.items():\n","            results.append({\n","                'model': model_class.__name__,\n","                'embedding_dim': embedding_dim,\n","                'k': k,\n","                'recall': metrics['recall'],\n","                'precision': metrics['precision'],\n","                'hit_ratio': metrics['hit_ratio'],\n","                'epoch': metrics['epoch'],\n","                'lr': lr\n","            })\n","    \n","    # Convert results to DataFrame\n","    df = pd.DataFrame(results)\n","    \n","    # Save results to CSV\n","    filename = f\"{model_class.__name__}_embedding_dim_tuning_results.csv\"\n","    df.to_csv(filename, index=False)\n","    print(f\"Results saved to {filename}\")\n","\n","# 임베딩 차원 값들 정의\n","embedding_dims = [64, 128, 256, 896]\n","embedding_dims_text = [64, 128, 256]\n","\n","# 기본 LightGCN에 대한 임베딩 차원 튜닝 실행\n","hyperparameter_tuning_embedding_dim(LightGCN, yelp, embedding_dims, num_epochs=100, batch_size=32, lr=best_lr_lightgcn, text=False)\n","\n","# TextLightGCN에 대한 임베딩 차원 튜닝 실행\n","hyperparameter_tuning_embedding_dim(TextLightGCN, yelp, embedding_dims_text, num_epochs=300, batch_size=32, lr=best_lr_textlightgcn, text=True)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPRS6l6zpm9ig0MLe3gXML7","gpuType":"T4","machine_shape":"hm","mount_file_id":"1CPcP-h4D76jF5vif69f0MbiI7d0EStg1","provenance":[{"file_id":"1bfbwaQUAnk4oyXLrqSLkFBfOuYzdRFv2","timestamp":1720783005695},{"file_id":"1Jni6b5_iQhcsO-Y3mRS3Tb8oAxiBQE1E","timestamp":1720494651406}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
